{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "請閱讀相關文獻，並回答下列問題：\n",
    "\n",
    "\n",
    "\n",
    "脊回歸 (Ridge Regression)\n",
    "Linear, Ridge, Lasso Regression 本質區別\n",
    "\n",
    "\n",
    "作業１：LASSO 回歸可以被用來作為 Feature selection 的工具，請了解 LASSO 模型為什麼可用來作 Feature selection\n",
    "Lesso與嶺迴歸非常相似，都是在迴歸優化函式中增加了一個偏置項以減少共線性的影響，從而減少模型方程。不同的是Lasso迴歸中使用了絕對值偏差作為正則化項，Lasso迴歸可以表示成下面的式子： \n",
    " min||Xw−y||2 Z||w||” role=”presentation” style=”position: relative;”> min||Xw−y||2 Z||w|| min||Xw−y||2 Z||w||\\ min||Xw-y||^2 Z||w|| \n",
    "嶺迴歸和Lasso迴歸之間的差異可以歸結為L1正則和L2正則之間的差異： \n",
    "內建的特徵選擇(Built-in feature selection):這是L1範數很有用的一個屬性，二L2範數不具有這種特性。因為L1範數傾向於產生係數係數。例如，模型中有100個係數，但其中只有10個係數是非零係數，也就是說只有這10個變數是有用的，其他90個都是沒有用的。而L2範數產生非稀疏係數，所以沒有這種屬性。因此可以說Lasso迴歸做了一種引數選擇形式，未被選中的特徵變數對整體的權重為0。 \n",
    "稀疏性：指矩陣或向量中只有極少個非零係數。L1範數具有產生具有零值或具有很少大系數的非常小值的許多係數的屬性。 \n",
    "計算效率：L1範數咩有解析解，但L2範數有。這使得L2範數的解可以通過計算得到。L1範數的解具有稀疏性，這使得它可以與稀疏演算法一起使用，這使得在計算上更有效率。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "作業２：當自變數 (X) 存在高度共線性時，Ridge Regression 可以處理這樣的問題嗎?\n",
    "\n",
    "分析嶺迴歸之前首先要說的一個共線性(collinearity)的概念，共線性是自變數之間存在近似線性的關係，這種情況下就會對迴歸分析帶來很大的影響。因為迴歸分析需要我們瞭解每個變數與輸出之間的關係，高共線性就是說自變數間存在某種函式關係，如果兩個自變數（X1和X2）之間存在函式關係，那麼當X1改變一個單位時，X2也會相應的改變，這樣就沒辦法固定其他條件來對單個變數對輸出的影響進行分析了，因為所分析的X1總是混雜了X2的作用，這樣就造成了分析誤差，所以迴歸分析時需要排除高共線性的影響。 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
